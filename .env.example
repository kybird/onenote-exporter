# [Option 2] Groq API Key (Fallback)
# If Antigravity fails (Quota Exhausted), the system will try this.
# GROQ_API_KEY=YOUR_GROQ_API_KEY_HERE

# Optional: Override Model
# Default for Groq Fallback: llama-3.3-70b-versatile
# GROQ_LLM_MODEL=llama-3.3-70b-versatile
#
# [Option 3] Local LLM (llama.cpp server) - 2nd Fallback
# If both Antigravity and Groq fail.
# LOCAL_LLM_URL=http://localhost:8080/v1
# LOCAL_LLM_MODEL=local-model
# LLM_PROVIDER=local (default: antigravity)
# Options: antigravity, groq, local
#
# [Testing/Override]
# Force specific provider:
# LLM_PROVIDER=local
#
# Prevent state.json updates (Dry Run):
# SKIP_STATE_UPDATE=true
#
# Antigravity Models (High Quota):
# LLM_MODEL=models/antigravity-gemini-3-pro
# LLM_MODEL=models/antigravity-gemini-3-flash
# LLM_MODEL=models/antigravity-claude-sonnet-4-5
#
# Default Active Model:
# LLM_MODEL=models/antigravity-gemini-3-pro

# [Configuration Mode]
# You can set the model and mode separately.
#
# 1. Select Base Model
# LLM_MODEL=models/antigravity-gemini-3-pro
#
# 2. Select Mode (Optional)
# LLM_MODE=plan   -> High Reasoning (Thinking)
# LLM_MODE=fast   -> Low Latency (Minimal Thinking)
